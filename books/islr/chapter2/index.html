<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
  <head>
    

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

  <title>Chapter2: Basic concepts of Statistical Learning - Deep Trace</title>
  <meta property="og:title" content="Chapter2: Basic concepts of Statistical Learning" />
  <meta name="twitter:title" content="Chapter2: Basic concepts of Statistical Learning" />
  <meta name="description" content="Statistical Learning What is statistial learning Suppose we observe a quantitative response $Y$ and $p$ different predictors, $X_1, X_2, \ldots,X_p$ . We assume that there is a relationship between $Y$ and $X=(X_1, X_2,\ldots,X_p)$, which can be written as
 $$ Y=f(X)&#43;\epsilon $$  Here $f$ is some fixed but unknown fucntion of $X_1,X_2,\ldots,X_p$, and $\epsilon$ is a random error term, which is independent of $X$ and has mean zero. In this formulation, $f$ represents the systematic informationa $X$ provides about $Y$.">
  <meta property="og:description" content="Statistical Learning What is statistial learning Suppose we observe a quantitative response $Y$ and $p$ different predictors, $X_1, X_2, \ldots,X_p$ . We assume that there is a relationship between $Y$ and $X=(X_1, X_2,\ldots,X_p)$, which can be written as
 $$ Y=f(X)&#43;\epsilon $$  Here $f$ is some fixed but unknown fucntion of $X_1,X_2,\ldots,X_p$, and $\epsilon$ is a random error term, which is independent of $X$ and has mean zero. In this formulation, $f$ represents the systematic informationa $X$ provides about $Y$.">
  <meta name="twitter:description" content="Statistical Learning What is statistial learning Suppose we observe a quantitative response $Y$ and $p$ different predictors, $X_1, X_2, \ldots,X_p$ . We assume that there is a relationship between …">
  <meta name="author" content="S Wang"/><script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "WebSite",
    "name": "Deep Trace",
    
    "url": "https://swang8.github.io"
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "",
  "url": "https://swang8.github.io"
  
  
  
  
}
</script>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
        "@type": "ListItem",
        "position": 1,
        "item": {
          "@id": "https://swang8.github.io",
          "name": "home"
        }
    },{
        "@type": "ListItem",
        "position": 3,
        "item": {
          "@id": "https://swang8.github.io/books/islr/chapter2/",
          "name": "Chapter2 basic concepts of statistical learning"
        }
    }]
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "author": {
    "name" : "S Wang"
  },
  "headline": "Chapter2: Basic concepts of Statistical Learning",
  "description" : "Statistical Learning What is statistial learning Suppose we observe a quantitative response $Y$ and $p$ different predictors, $X_1, X_2, \ldots,X_p$ . We assume that there is a relationship between $Y$ and $X=(X_1, X_2,\ldots,X_p)$, which can be written as
 $$ Y=f(X)&#43;\epsilon $$  Here $f$ is some fixed but unknown fucntion of $X_1,X_2,\ldots,X_p$, and $\epsilon$ is a random error term, which is independent of $X$ and has mean zero. In this formulation, $f$ represents the systematic informationa $X$ provides about $Y$.",
  "inLanguage" : "en",
  "wordCount": 2923,
  "datePublished" : "2019-02-01T14:46:09",
  "dateModified" : "2019-02-01T14:46:09",
  "image" : "https://swang8.github.io/images/mollusc01.png",
  "keywords" : [ "islr, statistics learning, R" ],
  "mainEntityOfPage" : "https://swang8.github.io/books/islr/chapter2/",
  "publisher" : {
    "@type": "Organization",
    "name" : "https://swang8.github.io",
    "logo" : {
        "@type" : "ImageObject",
        "url" : "https://swang8.github.io/images/mollusc01.png",
        "height" :  60 ,
        "width" :  60
    }
  }
}
</script>

<meta property="og:title" content="Chapter2: Basic concepts of Statistical Learning" />
<meta property="og:description" content="Statistical Learning What is statistial learning Suppose we observe a quantitative response $Y$ and $p$ different predictors, $X_1, X_2, \ldots,X_p$ . We assume that there is a relationship between $Y$ and $X=(X_1, X_2,\ldots,X_p)$, which can be written as
 $$ Y=f(X)&#43;\epsilon $$  Here $f$ is some fixed but unknown fucntion of $X_1,X_2,\ldots,X_p$, and $\epsilon$ is a random error term, which is independent of $X$ and has mean zero. In this formulation, $f$ represents the systematic informationa $X$ provides about $Y$.">
<meta property="og:image" content="https://swang8.github.io/images/mollusc01.png" />
<meta property="og:url" content="https://swang8.github.io/books/islr/chapter2/" />
<meta property="og:type" content="website" />
<meta property="og:site_name" content="Deep Trace" />
  <meta name="twitter:title" content="Chapter2: Basic concepts of Statistical Learning" />
  <meta name="twitter:description" content="Statistical Learning What is statistial learning Suppose we observe a quantitative response $Y$ and $p$ different predictors, $X_1, X_2, \ldots,X_p$ . We assume that there is a relationship between …">
  <meta name="twitter:image" content="https://swang8.github.io/images/mollusc01.png" />
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="@shichenTX" />
  <meta name="twitter:creator" content="@shichenTX" />
  <link href='https://swang8.github.io/images/favicon.ico' rel='icon' type='image/x-icon'/>
  <meta property="og:image" content="https://swang8.github.io/images/mollusc01.png" />
  <meta name="twitter:image" content="https://swang8.github.io/images/mollusc01.png" />
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="@shichenTX" />
  <meta name="twitter:creator" content="@shichenTX" />
  <meta property="og:url" content="https://swang8.github.io/books/islr/chapter2/" />
  <meta property="og:type" content="website" />
  <meta property="og:site_name" content="Deep Trace" />

  <meta name="generator" content="Hugo 0.53" />
  <link rel="alternate" href="https://swang8.github.io/index.xml" type="application/rss+xml" title="Deep Trace">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
  <link rel="stylesheet" href="https://swang8.github.io/css/main.css" /><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" />
  <link rel="stylesheet" href="https://swang8.github.io/css/highlight.min.css" /><link rel="stylesheet" href="https://swang8.github.io/css/codeblock.css" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css" integrity="sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css" integrity="sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
               delimiters: [
                   {left: "$$", right: "$$", display: true},
                   {left: "\\[", right: "\\]", display: true},
                   {left: "$", right: "$", display: false},
                   {left: "\\(", right: "\\)", display: false}
               ]
        });
    });
</script>




  </head>
  <body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="https://swang8.github.io">Deep Trace</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
        
          
            <li>
              <a title="Blog" href="/blog/">Blog</a>
            </li>
          
        
          
            <li class="navlinks-container">
              <a class="navlinks-parent">Books</a>
              <div class="navlinks-children">
                
                  <a href="/books/islr">ISLR</a>
                
                  <a href="/books/mla">Machine Learning in Action</a>
                
              </div>
            </li>
          
        
          
            <li>
              <a title="About" href="/about/">About</a>
            </li>
          
        
          
            <li>
              <a title="Tags" href="/tags">Tags</a>
            </li>
          
        

        

        
      </ul>
    </div>

    
      <div class="avatar-container">
        <div class="avatar-img-border">
          <a title="Deep Trace" href="https://swang8.github.io">
            <img class="avatar-img" src="https://swang8.github.io/images/mollusc01.png" alt="Deep Trace" />
          </a>
        </div>
      </div>
    

  </div>
</nav>




    


<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>


  
  
  






  

  <header class="header-section ">
    
    <div class="intro-header no-img">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="books-heading">
              
                <h1>Chapter2: Basic concepts of Statistical Learning</h1>
              
              
                <hr class="small">
              
              
              
            </div>
          </div>
        </div>
      </div>
    </div>
  </header>


    
<div class="container" role="main">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
      <article role="main" class="blog-post">
        

<h1 id="statistical-learning">Statistical Learning</h1>

<h2 id="what-is-statistial-learning">What is statistial learning</h2>

<p>Suppose we observe a quantitative response $Y$ and $p$ different predictors, $X_1, X_2, \ldots,X_p$ . We assume that there is a relationship between $Y$ and $X=(X_1, X_2,\ldots,X_p)$, which can be written as</p>

<div>
$$
Y=f(X)+\epsilon
$$
</div>

<p>Here $f$ is some fixed but unknown fucntion of $X_1,X_2,\ldots,X_p$, and $\epsilon$ is a random <code>error term</code>, which is independent of $X$ and has mean <code>zero</code>. In this formulation, $f$ represents the <code>systematic</code> informationa $X$ provides about $Y$.</p>

<h3 id="why-estimate-f">Why estimate $f$ ?</h3>

<p>Two reasons: <strong>prediction</strong> and <strong>inference</strong>.</p>

<h4 id="u-prediction-u"><u>Prediction</u></h4>

<p>In many cases, a set of input $X$ are readily available, but the output $Y$ can not be easily obtained. In this setting, since the error term averages to zero, we can predict $Y$ using</p>

<div>
$$
\hat{Y} = \hat{f}(X)
$$
</div>

<p>Where $\hat{f}$ represents our estimate for $f$, and $\hat{Y}$ represents the resulting predictio for $Y$. In this setting, $\hat{f}$ is oftern treated as a <em>black box</em>, in the sense that one is not typically concerned with the exact form of $\hat{f}$, provided that it yields accurate predictions for $Y$.</p>

<p><code>reducible error</code> and <code>irreducible error</code></p>

<p>The accuray of $\hat{Y}$ as prediction for $Y$ depends on two quantities, which we will call them <em>reducible error</em> and <em>irreducible error</em>. In general, $\hat{f}$ will not be a perfect estimate for $f$, and this inaccuracy will introduce some error. This error is <em>reducible</em> because we can potentially improve the accuracy of $\hat{f}$ by using the most appropriate statistical learning techs to estimate $f$. However, even if it were possible to form a perfect etimate for $f$, so that our estimated response took the form $\hat{Y}=f(X)$, our prediction would still have some error in it!! <strong>Why?</strong> This is because $Y$ is also a function of $\epsilon$, which, by definition, cannot be predicted using $X$. Therefore, variables associated with $\epsilon$ also affects the accuracy of our predictions. This is known as the <em>irreducible</em> error.</p>

<p>Why is the irreducible error larger than zero?
The quantify $\epsilon$ may contain unmeasured variables that are useful in predicting $Y$: since we don&rsquo;t measure them, $f$ cannot use them for its prediction. The euatity $\epsilon$ may also contain unmeasurable variation. For example, the risk of an adverse reaction might vary for a given patient on a given ay, depending on manufacturing variation in the drug itself or the patient&rsquo;s general feeling of well-being on that day.</p>

<p>Consider a given estimate $\hat{f}$ and a set of predictors $X$, which yields the prediction $\hat{Y}=\hat{f}(X)$. Assume for a moment that both $\hat{f}$ and $X$ are fixed. Then, it is easy to show that</p>

<div>
$$
\begin{aligned}
E(Y-\hat{Y})^2 &= E[f(X) + \epsilon - \hat{f}(X)]^2  \\
&=\underbrace{[f(X) - \hat{f}(X)]^2}_{reducible} + \underbrace{Var(\epsilon)}_{irreducible} 
\end{aligned}
$$
</div>

<p>Where $E(Y - \hat{Y})^2$ represents the average, or <code>expected value</code>, of the squared difference between the predicted and actual value of $Y$, and Var($\epsilon$) represents the <strong>variance</strong> associated with the error term $\epsilon$.</p>

<p>Keep in mind that the <strong>irreducible error</strong> will always provide an upper bound on hte accuracy of our prediction for $Y$. The focus would be estimating of $f$ with the aim of minimizing the <strong>reducible error</strong>.</p>

<h4 id="u-inference-u"><u>Inference</u></h4>

<p>We are often interested in understanding the way that $Y$ is affected as $X_1,X_2,\ldots,X_p$ change. In this situation we wish to estimate $f$, but our goal is not necessarily to make predictions for $Y$. We instead want to understand the relationship between $X$ and $Y$. More specifically, to understand how $Y$ changes as a function of $X_1,X_2,\ldots,X_p$.</p>

<p>Now $f$ cannot be treated as a <em>black box</em>, because we need to know its exact form. In this setting, one may be interested in answering the following questions:</p>

<ul>
<li><p><em><u>Which predictors are associated with the response?</u></em> It&rsquo;s often the case that only a small fraction of the repdictors are substantially associated with $Y$. Identifying a few <em>important</em> predictors among a large set of possible variables can be very useful.</p></li>

<li><p><em><u>What is the relationship between the response and each predictor?</u></em> Some predictors may have a positive relationship with $Y$, while others may have opposite relationships. Depending on the complexity of $f$, the relationship between the response and a given predictor may also depend on the valurs of the other predictors.</p></li>

<li><p><em><u>Can the relationship between $Y$ and each predictor be adequately summarized using a linear equation, or is the relatioship more complicated?</u></em> Historically, most mthods for estimating $f$ have taken a linear form. In some situations, such an assumption is resonable or even desirable. But often the true relationship is more complicated, in which case a linear model may not provide an accurate representation of the relationship between the input and output variables.</p></li>
</ul>

<h3 id="how-do-we-estimate-f">How do we estimate $f$ ?</h3>

<p>We will explore many linear and non-linear approaches for estimating $f$. However, these methods generally share certain characteristics.</p>

<p><strong>Training data</strong>, data that will be used to train, or teach, our method how to estimate $f$.</p>

<p>The goal is to apply a statistical learning method to the <em>training data</em> in order to estimate the unknown function $f$. In other words, we want to find a funtion $\hat{f}$ that $Y\approx\hat{f}(X)$ for any given observation $(X, Y)$.</p>

<p>Broadly speaking, most statitical learning methods for this task can be characterized as either <code>parametric</code> or <code>non-parametric</code>.</p>

<h4 id="u-parametric-methods-u"><u>Parametric Methods</u></h4>

<p>Parametric methods involve a two-step model-based approach.</p>

<ol>
<li>First, we make an assumption about the functional form, or shape, of $f$. For example, one very simple assumption is that $f$ is linear in $X$:</li>
</ol>

<div>
$$
f(X)=\beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} + \cdots+\beta_{p}X_{p}
$$
</div>

<p>This is <em>linear model</em>. Once we have assumed that $f$ is linear, the problem of estimation $f$ is greatly simplified. Instead of having to estimate an entirely arbitrary p-dimentional function $f(X)$, one only needs to estimate the <em>p+1</em> coefficients $\beta$.</p>

<ol>
<li>After a model has been selected, we need a procedure that uses the training data to <em>fit</em> or <em>train</em> the model. In the case of the linear model, we need to estimate the parameters $\beta<em>{0}$ , $\beta</em>{1}$ , $\ldots$ , $\beta_{p}$. That is, we want to find values of these parameters such that</li>
</ol>

<div>
$$
Y \approx \beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} + \cdots + \beta_{p}X_{p}
$$
</div>

<p>The most common approach to fitting the model is refered to as <em>(ordinary) least squares</em>.</p>

<p>The model-based approach just described is refered to as <em>parametric</em>; it reduces the problem ofestimating $f$ down to one of the estimating a set of parameters. Assuming a parametric form for $f$ simplifies the problem of estimating $f$ because it is generally much easier to estimate a set of parameters, such as $\beta_0, \beta_1, \ldots, \beta_p$  in the linear model, than it is to fit an entirely arbitrary function $f$ . The potentil <em>disadvantage</em> of a parametric approach is that the model we choose will usually not match the true unknown form of $f$. If the chosen model is too far from the true $f$, then our estimate will be poor. We can try to address this problem by choosing <em>flexible</em> models that can fit many different possible functional forms for $f$. But in general, fitting a more flexible model requires estimating a greater number of parameters. These more comlex models can lead to phenomenon known as <strong>overfitting</strong> the data, which essentially means they folow the errors, or <em>noise</em>, too closely.</p>

<h4 id="u-non-parametric-methods-u"><u>Non-parametric Methods</u></h4>

<p>Non-parametric methods do not make explicit assumptions about the duncitonal forms of $f$. Instead they seek an estimate of $f$ that gets as close to the data points as possible without being too rough or wiggly.</p>

<p>Such approaches can have a major advantage over parametric approaches: by avoiding the assumption of a particular functional form for $f$, they have the potential to accurately fit a wider range of possible shapes for $f$.</p>

<p>Any parametric approach brings with it the possibility that the functional form used to estimate $f$ is very different from the true $f$, in which case the resulting model will not fit hte data well.</p>

<p>In contrast, non-parametric approaches completely avoid this danger, since essentially no assumption about the form of $f$ is mad. But non-parametric approaches do suffer from a major disadvantage: since they do not reduce the problem of estimating $f$ to a small number of parameters, a very large number of observations (far more than is typically needed for a parametric approace) is required in order to obtain and accurate estiamte for $f$.</p>

<h3 id="the-trade-off-between-prediction-accuracy-and-model-interpretability">The Trade-Off between prediction accuracy and Model interpretability</h3>

<p>More flexible model can generate a much wider range of possible shapes to estimate $f$.
<em>Why would we ever choose to use a more restrictive method instead of a very flexible approach?</em> The more restrictive models are much more interpretable.</p>

<p><img src="/images/ch2_flex_inter.png" alt="flexiblity" /></p>

<p>When inference is the goal, there are clear advantages to using simple and relatively inflexible statistical learning methods. In some seetings, however, we are only interested in prediciton and teh interpretability of the prediction model is simply not of interest. For exampl,e if we seek to develop an algorithm to predict the price of a stock, our sole requirement for the requirement for the algorithm to predict the price accurately &ndash; interpretability is not a concern. In this case, we might expect that it will be best to use the most flexible model available. Surprisely, this is not always the case! We will often obtain more accurate predictions using a less flexible method. This phenomenon, which may seem couterintuitive at first glace, has to do with the potential for <em>overfitting</em> in highly flexible methods.</p>

<h3 id="supervised-vs-unsupervised-learning">Supervised VS Unsupervised Learning</h3>

<p>Most statistical learning problems fall into one of the two categories: <em>supervised</em> or <em>unsupervised</em>.</p>

<p><u>Supervised</u>: For each obervation of the predictor measurements $x_i, i=1,2,\ldots ,n$ there is an associated response measurement $y_i$. We wish to fit a model that relates the response to the predictors, with the aim of accurately predicting the response for future observations or better understanding the relationship between the response and the predictors. Many classical statistical learning methods such as linear regression and logistic regression, as well as more modern approaches such GAM, boosting, and support vector machines, operate in the supervised learning domain.</p>

<p><u>Unsupervised</u>: in contrast, unserpervised learning describes the somewhat more challenging situation in which for every obervation $i=1,2,\ldots, n$, we observe a vector of measurements $x_i$ but no associated response $y_i$. It is not possible to fit a linear regression model, since there is no response variable to predict. In this setting, we are in some sense working blind; the situation is referred to as <em>unsupervised</em> because we we lack a response variable that can supervise our analysis. What sort of analysis is possible? <em>cluster analysis</em>, the goal of cluster analysis is to ascertain, on the basis of $x_1, \ldots, x_n$, whether the observations fall into relatively distinct groups.</p>

<p>Many problems fall naturally into the <em>supervised</em> or <em>unsupervised</em> learning paradigms. However, sometimes the question of whether an analysis shoudl be considered supervised or unsupervised is less clear-cut. For instance, suppose that we have a set of $n$ observations. Fomr $m$ of hte observations, where $m &lt; n$, we have both predictor measurements and a response measurement. For the remaining $n - m$ obervations, we have predictor measurements but no response measurement. SUch a scenario can arise if the predictors can be measured relatively cheaplly but hte corresponding response are much more expensive to collect. We reger to this setting as a <em>semi-supervised</em> learning problem. In this setting, we wish to use a statistical learning method that can incorporate the $m$ obervations for which response measurements are available as well as the $n-m$ observations for which they are not.</p>

<h3 id="regression-vs-classification-problems">Regression VS Classification problems</h3>

<p>Variables can be characterized as either <em>quantitative</em> or <em>qualitative</em> (also known as <em>categorical</em>). We tend to refer to problems with a quantitative response as <em>regression</em> problems, which those involving a qualitative response are ofter refered to as <em>classification</em> problems. However, the distinction is not always that crisp. Least squares linear regression is used with a quantitative response, whereas logistic regression is typically used with a qualitative (two-class, or <em>binary</em>) response. As such it is oftern used as a classification method. But since it estimates class probabilities, it can be thought of as a regression method as well. Some statistical methods, such as K-nearest neighbors and boosting, can be used in the case of either quantitative or qualitative responses.</p>

<p>We tend to select statistical learning methods on the basis of whether the response is quantitative or qualitative; i.e. we might use linear regression when quantitative and logistic regression when qualitative. However, whether the <em>predictors</em> are qualitative or quantitative is gneerally condisered less important. Most of the statistical learning methods can be applied regardless of the predictor variable type, provided that any qualitative predictors are properly <em>coded</em> begore the analysis is performed.</p>

<h2 id="assessing-model-accuracy">Assessing Model Accuracy</h2>

<p><strong>There is no free lunch in statistics</strong>: no one method dominates all others over all possible data sets. Hence it is important to decie for any given set of data which method produces the best results. Selecting the best approach can be one of the most challenging parts of performing statistical learning in practice.</p>

<h3 id="measuring-the-quality-of-fit">Measuring the quality of Fit</h3>

<p>In order to evaluate the performance of a statistical learning method on a given data et, we need some way to meaure how well its predictions actually match the observed data. That is. we need to quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation. In the regression settting, the most commonly-used measure is the <code>mean squared error</code> (MSE), given by</p>

<div>
$$
MSE=\frac{1}{n}\sum_{j=0}^n (y_i-\hat{f}(x_i))^2
$$
</div>

<p>The MSE is computed using the training data that was used to fit the model, and so should more accurately be refered to as the <em>training MSE</em>. But in general, we do not really care how well the method works on the training data. Rather, <em>we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data.</em> Why? Think about training model to predict stock price.</p>

<p>Suppose that we fit our statistical learning method on our training observations ${(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)}$, and we obtain the estimate $\hat{f}$. We can then compute $\hat{f}(x_1), \hat{f}(x_2), \ldots. \hat{f}(x_n)$. If these are approximately equal to $y_1,y_2,\ldots,y_n$, then the training MSE is small. However, we are really not interested in whether $\hat{f}(x_i) \approx y_i$; instead, we want to know where $\hat{f}(x_0)$ is approximately equal to $y_0$, where $(x_0, y_0)$ is <strong>a previously unseen test observation not used to train the statistical learning method.</strong></p>

<p>We want to choose the method that gives the lowest <strong>test</strong> MSE, as opposed to the lowest training MSE. In other words, if we had a large number of test observations, we could compute</p>

<div>
$$
Ave(y_0 - \hat{f}(x_0))^2
$$
</div> 

<p>The average square prediction error for these test observations $(x_0, y_0)$.</p>

<h3 id="the-bias-variation-trade-off">The Bias-Variation Trade-Off</h3>

<p>For a given value $x_0$, the expected test MSE can be decomposed into the sum of three fundamental quantities: the <em>variance</em> of $\hat{f}(x_0)$, the squared <em>bias</em> of $\hat{f}(x_0)$ and the variance of the error term $\epsilon$. That is</p>

<div>
$$
E(y_0 - \hat{f}(x_0))^2 = Var(\hat{f}(x_0)) + [Bias(\hat{f}(x_0))]^2 + Var(\epsilon)
$$
</div>

<p>Here the notation $E(y_0-\hat{f}(x_0))^2$ defines the <em>exptected test MSE</em>, and refers to the average test MSE that we would obtain if we repeatedly estimated $f$ using a large number of training sets, and tested each at $x_0$. The overall expected test MSE can be computed by averaging $E(y_0-\hat{f}(x_0))^2$ overall possible values of $x_0$ in the test set.</p>

<p>The equation shows that in order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves <em>low variance</em> and <em>low bias</em>.</p>

<p>What do we mean by the the <code>variance</code> and <code>bias</code> of a statistical learning method?</p>

<p><code>Variance</code> referes to the amount of by which $\hat{f}$ would change if we estimated it using a different training data set. Since the training data are used to fit the statistical method, different training data sets will result in a different $\hat{f}$. But ideally the estimate of $f$ should not vary too much between training sets.</p>

<p>If a method has high variance then small changes in the training data can result in large changes in $\hat{f}$. In general, more flexible statistical methods have higher variance.</p>

<p><code>Bias</code> referes to theerro that is introduced by approximating a real-life problem, which may be extremely complicated, by as much simpler model.</p>

<p>As a general ruls, as we use more flexible methods, the variance will increase and the bias will decrease.</p>

<h3 id="the-classification-setting">The Classification Setting</h3>

<p>Suppose that we seek to estimate $f$ on the basis of training observations ${(x_1, y_1), \ldots, (x_n, y_n)}$, where now $y_1,\ldots,y_n$ are qualitative. The training <em>error rate</em> is calculated as the proportion of mistakes that are made if we apply our estimte $\hat{f}$ to the training observations:</p>

<div>
$$
\frac{1}{n}\sum_{i=1}^n I(y_i \neq \hat{y}_0)
$$
</div>

<p>Here $\hat{y}_i$ is the predicted class label for the <em>ith</em> observation using $\hat{f}$. And $I(y_i \neq \hat{y}_i)$ is an <em>indicator variable</em> that equals 1 if $y_i \neq \hat{y}_i$ and zero if $y_i = \hat{y}_i$.</p>

<p>As in the regression setting, we are most interested in the error rates that result from applying our classifier to test observations that were not used in training. The <em>test error rate</em> associated with a set of test observations of the form $(x_0, y_0)$ is given by</p>

<div>
$$
Ave(I(y_0 \neq \hat{y}_0))
$$
</div>

<h4 id="the-bayes-classifier">The Bayes Classifier</h4>

<p>To minimize the test error rate: assign each observation to the most likely class given its predictor values.</p>

<p>The conditional probability:</p>

<div>
$$
Pr(Y=j|X=x_0)
$$
</div>

<p>It is the probability that $Y = j$, given the observed predictor vector $x_0$. This very simple classifier is called the <em><code>bayes classifier</code></em>.</p>


        
          <div class="blog-tags">
            
              <a href="https://swang8.github.io/tags/islr/">islr</a>&nbsp;
            
              <a href="https://swang8.github.io/tags/statistics-learning/">statistics learning</a>&nbsp;
            
              <a href="https://swang8.github.io/tags/r/">R</a>&nbsp;
            
          </div>
        

        
            <hr/>
            <section id="social-share">
              <div class="list-inline footer-links">
                

<div class="share-box" aria-hidden="true">
    <ul class="share">
      
      <li>
        <a href="//twitter.com/share?url=https%3a%2f%2fswang8.github.io%2fbooks%2fislr%2fchapter2%2f&amp;text=Chapter2%3a%20Basic%20concepts%20of%20Statistical%20Learning&amp;via=shichenTX" target="_blank" title="Share on Twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//plus.google.com/share?url=https%3a%2f%2fswang8.github.io%2fbooks%2fislr%2fchapter2%2f" target="_blank" title="Share on Google Plus">
          <i class="fab fa-google-plus"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fswang8.github.io%2fbooks%2fislr%2fchapter2%2f" target="_blank" title="Share on Facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//reddit.com/submit?url=https%3a%2f%2fswang8.github.io%2fbooks%2fislr%2fchapter2%2f&amp;title=Chapter2%3a%20Basic%20concepts%20of%20Statistical%20Learning" target="_blank" title="Share on Reddit">
          <i class="fab fa-reddit"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2fswang8.github.io%2fbooks%2fislr%2fchapter2%2f&amp;title=Chapter2%3a%20Basic%20concepts%20of%20Statistical%20Learning" target="_blank" title="Share on LinkedIn">
          <i class="fab fa-linkedin"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.stumbleupon.com/submit?url=https%3a%2f%2fswang8.github.io%2fbooks%2fislr%2fchapter2%2f&amp;title=Chapter2%3a%20Basic%20concepts%20of%20Statistical%20Learning" target="_blank" title="Share on StumbleUpon">
          <i class="fab fa-stumbleupon"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.pinterest.com/pin/create/button/?url=https%3a%2f%2fswang8.github.io%2fbooks%2fislr%2fchapter2%2f&amp;description=Chapter2%3a%20Basic%20concepts%20of%20Statistical%20Learning" target="_blank" title="Share on Pinterest">
          <i class="fab fa-pinterest"></i>
        </a>
      </li>
    </ul>
  </div>
  
              </div>
            </section>
        

        
          
          
        
      </article>

      
        <ul class="pager blog-pager">
          
            <li class="previous">
              <a href="https://swang8.github.io/books/islr/intro/" data-toggle="tooltip" data-placement="top" title="Introduction">&larr; Previous Post</a>
            </li>
          
          
            <li class="next">
              <a href="https://swang8.github.io/books/islr/chapter3/" data-toggle="tooltip" data-placement="top" title="Chapter3: Linear Regression">Next Post &rarr;</a>
            </li>
          
        </ul>
      


      
        
        
      

    </div>
  </div>
</div>

    <footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
              <li>
                <a href="mailto:shichen.wang@gmail.com" title="Email me">
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
                <a href="https://github.com/swang8" title="GitHub">
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
                <a href="https://twitter.com/shichenTX" title="Twitter">
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
          
          <li>
            
            <a href="https://swang8.github.io/index.xml" title="RSS">
            
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="fas fa-rss fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
        </ul>
        <p class="credits copyright text-muted">
          
            
              S Wang
            
          

          &nbsp;&bull;&nbsp;&copy;
          
            2019 - 2019
          

          
            &nbsp;&bull;&nbsp;
            <a href="https://swang8.github.io">Deep Trace</a>
          
        </p>
        
        <p class="credits theme-by text-muted">
          <a href="http://gohugo.io">Hugo v0.53</a> powered &nbsp;&bull;&nbsp; Theme by <a href="http://deanattali.com/beautiful-jekyll/">Beautiful Jekyll</a> adapted to <a href="https://github.com/halogenica/beautifulhugo">Beautiful Hugo</a>
          
        </p>
      </div>
    </div>
  </div>
</footer>

<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"></script>
<script src="https://code.jquery.com/jquery-1.12.4.min.js" integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<script src="https://swang8.github.io/js/main.js"></script>
<script src="https://swang8.github.io/js/highlight.min.js"></script>
<script> hljs.initHighlightingOnLoad(); </script>
<script> $(document).ready(function() {$("pre.chroma").css("padding","0");}); </script><script> renderMathInElement(document.body); </script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js" integrity="sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js" integrity="sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q" crossorigin="anonymous"></script>
<script src="https://swang8.github.io/js/load-photoswipe.js"></script>








  </body>
</html>

