<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
  <head>
    

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

  <title>Chapter3: Linear Regression - Deep Trace</title>
  <meta property="og:title" content="Chapter3: Linear Regression" />
  <meta name="twitter:title" content="Chapter3: Linear Regression" />
  <meta name="description" content="Linear Regression Linear regression is a very simple supervised learning methods, though still very useful.
Simple Linear Regression Simple linear regression is a straightforward approach for predicting a quantitative response $Y$ on the basis of a single predictor variable $X$. It assumes that there is approximately a linear relationship between $X$ and $Y$.
 $$ Y \approx \beta_{0} &#43; \beta_{1}X $$  In the equation, $\beta_0$ and $\beta_1$ are two unknown constants that represetn the intercept and slope termes in the linear model.">
  <meta property="og:description" content="Linear Regression Linear regression is a very simple supervised learning methods, though still very useful.
Simple Linear Regression Simple linear regression is a straightforward approach for predicting a quantitative response $Y$ on the basis of a single predictor variable $X$. It assumes that there is approximately a linear relationship between $X$ and $Y$.
 $$ Y \approx \beta_{0} &#43; \beta_{1}X $$  In the equation, $\beta_0$ and $\beta_1$ are two unknown constants that represetn the intercept and slope termes in the linear model.">
  <meta name="twitter:description" content="Linear Regression Linear regression is a very simple supervised learning methods, though still very useful.
Simple Linear Regression Simple linear regression is a straightforward approach for …">
  <meta name="author" content="S Wang"/><script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "WebSite",
    "name": "Deep Trace",
    
    "url": "https://swang8.github.io"
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "",
  "url": "https://swang8.github.io"
  
  
  
  
}
</script>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
        "@type": "ListItem",
        "position": 1,
        "item": {
          "@id": "https://swang8.github.io",
          "name": "home"
        }
    },{
        "@type": "ListItem",
        "position": 3,
        "item": {
          "@id": "https://swang8.github.io/books/islr/chapter3/",
          "name": "Chapter3 linear regression"
        }
    }]
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "author": {
    "name" : "S Wang"
  },
  "headline": "Chapter3: Linear Regression",
  "description" : "Linear Regression Linear regression is a very simple supervised learning methods, though still very useful.
Simple Linear Regression Simple linear regression is a straightforward approach for predicting a quantitative response $Y$ on the basis of a single predictor variable $X$. It assumes that there is approximately a linear relationship between $X$ and $Y$.
 $$ Y \approx \beta_{0} &#43; \beta_{1}X $$  In the equation, $\beta_0$ and $\beta_1$ are two unknown constants that represetn the intercept and slope termes in the linear model.",
  "inLanguage" : "en",
  "wordCount": 4147,
  "datePublished" : "2019-02-07T14:46:09",
  "dateModified" : "2019-02-07T14:46:09",
  "image" : "https://swang8.github.io/images/mollusc01.png",
  "keywords" : [ "islr, statistics learning, R" ],
  "mainEntityOfPage" : "https://swang8.github.io/books/islr/chapter3/",
  "publisher" : {
    "@type": "Organization",
    "name" : "https://swang8.github.io",
    "logo" : {
        "@type" : "ImageObject",
        "url" : "https://swang8.github.io/images/mollusc01.png",
        "height" :  60 ,
        "width" :  60
    }
  }
}
</script>

<meta property="og:title" content="Chapter3: Linear Regression" />
<meta property="og:description" content="Linear Regression Linear regression is a very simple supervised learning methods, though still very useful.
Simple Linear Regression Simple linear regression is a straightforward approach for predicting a quantitative response $Y$ on the basis of a single predictor variable $X$. It assumes that there is approximately a linear relationship between $X$ and $Y$.
 $$ Y \approx \beta_{0} &#43; \beta_{1}X $$  In the equation, $\beta_0$ and $\beta_1$ are two unknown constants that represetn the intercept and slope termes in the linear model.">
<meta property="og:image" content="https://swang8.github.io/images/mollusc01.png" />
<meta property="og:url" content="https://swang8.github.io/books/islr/chapter3/" />
<meta property="og:type" content="website" />
<meta property="og:site_name" content="Deep Trace" />
  <meta name="twitter:title" content="Chapter3: Linear Regression" />
  <meta name="twitter:description" content="Linear Regression Linear regression is a very simple supervised learning methods, though still very useful.
Simple Linear Regression Simple linear regression is a straightforward approach for …">
  <meta name="twitter:image" content="https://swang8.github.io/images/mollusc01.png" />
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="@shichenTX" />
  <meta name="twitter:creator" content="@shichenTX" />
  <link href='https://swang8.github.io/images/favicon.ico' rel='icon' type='image/x-icon'/>
  <meta property="og:image" content="https://swang8.github.io/images/mollusc01.png" />
  <meta name="twitter:image" content="https://swang8.github.io/images/mollusc01.png" />
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="@shichenTX" />
  <meta name="twitter:creator" content="@shichenTX" />
  <meta property="og:url" content="https://swang8.github.io/books/islr/chapter3/" />
  <meta property="og:type" content="website" />
  <meta property="og:site_name" content="Deep Trace" />

  <meta name="generator" content="Hugo 0.53" />
  <link rel="alternate" href="https://swang8.github.io/index.xml" type="application/rss+xml" title="Deep Trace">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
  <link rel="stylesheet" href="https://swang8.github.io/css/main.css" /><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" />
  <link rel="stylesheet" href="https://swang8.github.io/css/highlight.min.css" /><link rel="stylesheet" href="https://swang8.github.io/css/codeblock.css" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css" integrity="sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css" integrity="sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
               delimiters: [
                   {left: "$$", right: "$$", display: true},
                   {left: "\\[", right: "\\]", display: true},
                   {left: "$", right: "$", display: false},
                   {left: "\\(", right: "\\)", display: false}
               ]
        });
    });
</script>




  </head>
  <body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="https://swang8.github.io">Deep Trace</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
        
          
            <li>
              <a title="Blog" href="/blog/">Blog</a>
            </li>
          
        
          
            <li class="navlinks-container">
              <a class="navlinks-parent">Books</a>
              <div class="navlinks-children">
                
                  <a href="/books/islr">ISLR</a>
                
                  <a href="/books/mla">Machine Learning in Action</a>
                
              </div>
            </li>
          
        
          
            <li>
              <a title="About" href="/about/">About</a>
            </li>
          
        
          
            <li>
              <a title="Tags" href="/tags">Tags</a>
            </li>
          
        

        

        
      </ul>
    </div>

    
      <div class="avatar-container">
        <div class="avatar-img-border">
          <a title="Deep Trace" href="https://swang8.github.io">
            <img class="avatar-img" src="https://swang8.github.io/images/mollusc01.png" alt="Deep Trace" />
          </a>
        </div>
      </div>
    

  </div>
</nav>




    


<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>


  
  
  






  

  <header class="header-section ">
    
    <div class="intro-header no-img">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="books-heading">
              
                <h1>Chapter3: Linear Regression</h1>
              
              
                <hr class="small">
              
              
              
            </div>
          </div>
        </div>
      </div>
    </div>
  </header>


    
<div class="container" role="main">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
      <article role="main" class="blog-post">
        

<h1 id="linear-regression">Linear Regression</h1>

<p><em>Linear regression</em> is a very simple supervised learning methods, though still very useful.</p>

<h2 id="simple-linear-regression">Simple Linear Regression</h2>

<p><em>Simple linear regression</em> is a straightforward approach for predicting a quantitative response $Y$ on the basis of a single predictor variable $X$. It assumes that there is approximately a linear relationship between $X$ and $Y$.</p>

<div>
$$
Y \approx \beta_{0} + \beta_{1}X
$$
</div>

<p>In the equation, $\beta_0$ and $\beta_1$ are two unknown constants that represetn the <em>intercept</em> and <em>slope</em> termes in the linear model. Together, $\beta_0$ and $\beta_1$ are know as the model <code>coefficients</code> or <code>parameters</code>. Once we have used our training data to produce estimates $\hat{\beta}_0$ and $\hat{\beta}_1$ for the model coefficients, we can predict the response $\hat{y}$.</p>

<div>
$$
\hat{y} = \hat{\beta}_0 + \hat{\beta}_{1} x
$$
</div>

<h3 id="estimate-the-coefficients">Estimate the Coefficients</h3>

<p>In practic, $\beta_0$ and $\beta_1$ are unknown. So before we can use the equation to make predictions, we must use data to estimate the coefficients. Let</p>

<div>
$$
(x_1,y_1), (x_2, y_2), \ldots, (x_n, y_n)
$$
</div>

<p>represent $n$ observation pairs, each of which consists of a measurement of $X$ and a measurement of $Y$. Our goal is to obtain coefficient estimates $\hat{\beta}_0$ and $\hat{\beta}_1$ such that the linear model fits the avilable data well, that is, so that $y_i \approx \hat{\beta}_0 + \hat{\beta}_1 x_i$ for $i=1,2,\ldots,n$. In other words, we want to find an intercept $\hat{\beta}_0$ and a slope $\hat{\beta}_1$ such that the resulting line is as close a possible to the data points.</p>

<p>There are number of ways of measuing <code>closeness</code>. However, by far the most common approach involes minimiizng the <code>least squares</code> criterion.</p>

<p>Let $\hat{y} = \hat{\beta}_1 + \hat{\beta}_1 x_i$ be the prediction for $Y$ based on the *i*th values of $X$. The $e_i = y_i - \hat{y}_i$ represents the <em>ith</em> <code>residual</code> &ndash; this is the difference between the *i*th response value that is predicted by our linear model. We define the <code>residual sum of squares (RSS)</code> as</p>

<div>
$$
RSS=e_1^2 + e_2^2 + \ldots + e_n^2
$$
</div>

<p>or equivalently as
<div>
$$
RSS = (y_1 - \hat{\beta}_0 - \hat{\beta}_1 x_1)^2 + (y_2 - \hat{\beta}_0 - \hat{\beta}_1 x_2)^2 + \ldots + (y_n - \hat{\beta}_0 - \hat{\beta}_1 x_n)^2
$$
</div></p>

<p>The least squares approach choose $\hat{\beta}_0$ and $\hat{\beta}_1$ to minimize the RSS. Using some calculus, one can show that the minimizers are</p>

<div>
$$
\begin{aligned}
&\hat{\beta}_1 = \frac{\sum_{i=1}^n(x_i-\bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2},
\\
&\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x},\\
&where, \\
&\bar{y} \equiv \frac{1}{n} \sum_{i=1}^n y_i, \\
&\bar{x} \equiv \frac{1}{n} \sum_{i=1}^n x_i
\end{aligned}

$$
</div>

<p>The aforementioned formula defines the <code>least squares coefficient estimates</code> for simple linear regression.</p>

<h3 id="assessing-the-accuracy-of-the-coefficient-estimates">Assessing the accuracy of the Coefficient Estimates</h3>

<p>We assume that the <em>true</em> relationship between $X$ and $Y$ takes for thorm $Y=f(X)+\epsilon$ for some unknown function $f$, where $\epsilon$ is a mean-zero reandom error term. If $f$ is to be approximated by a liner function, then we can write this relationship as
<div>
$$
Y = \beta_0 + \beta_1X + \epsilon
$$
</div></p>

<p>The model given in the formula defines the <em>population regression line</em>, which is the best linear approximation to the true relationship between $X$ and $Y$. The least squares regression coefficient estimates characterize the <code>least square line</code>.</p>

<p>Using information from a sample to estimate characteristics of a large population. For example, suppose htat we are interested in knowing the population mean $\mu$ of some random variable $Y$. Unfortunately, $\mu$ is unknown, but we do have access to $n$ observations from $Y$, which we can use to estimate $\mu$. A reasonable estimate is $\hat{\mu} = \bar{y}$, where $\bar{y} = \frac{1}{n}\sum_{i=1}^ny_i$ is the sample mean. The sample mean and the population mean are different, but in general the sample mean will provide a good estimate of the population mean.</p>

<p>The analogy between linear regression and estimation of the mean of a random variable is an apt one based on the concept of <code>bias</code>. If we use the sample mean $\hat{\mu}$ to estimate $\mu$, this estimate is <code>unbiased</code>, in the sense that on average, we expect $\hat{\mu}$ to equal to $\mu$. It means, if we could average a huge numbr of estimates of $\mu$ obtained from a huge number of sets of observations, then this average would <em>exactly</em> equal to $\mu$. Hence, an unbiased estimator does not <em>systematically</em> over- or under-estimate the true parameter.</p>

<p>How accurate is hte sample mean $\hat{\mu}$ as an estimate of $\mu$?</p>

<p>We have established that the average of $\hat{\mu}$s over many data sets will be very close to $\mu$, but that a single estimate of $\hat{\mu}$ may be sunstantial underestimate or overestimate of $\mu$. How far off will that single estimate of $\hat{\mu}$ be? In general, we answer this question by computing the <code>standard error</code> of $\hat{\mu}$, written as $SE(\hat{\mu})$. We have the well-known formula</p>

<div>
$$
Var(\hat{\mu}) = SE(\hat{\mu})^2 = \frac{\sigma^2}{n}
$$
</div>

<p>Where $\sigma$ is the standard deviatioin of each of the realizations $y_i$ of $Y$. The equation wells us how this deviation shrinks with $n$ &ndash; the more observations we have, the smaller the standard error of $\hat{\mu}$.</p>

<p>The standard errors associated with $\hat{\beta}_0$ and $\hat{\beta}_1$ can be calculated as:</p>

<div>
$$
\begin{aligned}
&SE(\hat{\beta}_0)^2 = \sigma^2\Big[\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n (x_i - \bar{x})^2}\Big] \\
&SE(\hat{\beta}_1)^2 = \frac{\sigma^2}{\sum_{i=1}^n (x_i - \bar{x})^2}
\end{aligned}
$$
</div>

<p>Where $\sigma^2 = Var(\epsilon)$. For these formalas to be strictly valid, we need to assume that the error $\epsilon_i$ for each observation are uncorrelated with common variance $\sigma^2$. Though this might not be true for many cases, the formula still turs out to be a good approximation.</p>

<p>Notice in the formula, $SE(\hat{\beta}_1)$ is smaller when the $x_i$ are more spread out; intuitively we have more leverage to estimate a slope when this is the case. We can also see that $SE(\hat{\beta}_0)$ would be the same as $SE(\hat{\mu})$ if $\bar{x}$  were zero (in which case $\hat{\beta}_0$ would be equal to $\bar{y}$).</p>

<p>In gereral, $\sigma^2$ is not known, but can be estimated from the data. This estimation is known as the the <code>residual standard error</code>, and is given by the formula $RSE = \sqrt{RSS/(n-2)}$. Strictly speaking, when $\sigma^2$ is estimated from the data we should write $\widehat{SE}(\hat{\beta}_1)$ to indicate that an estimate has been made, but from simplicity of notation, we will drop the extra &ldquo;hat&rdquo;.</p>

<p>Standard errors can be used to compute <em>confidence intervals</em>. A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter. The range is defined in terms of lower and upper limits computed from the sample of data. For linear regression, the 95% confidence interval for $\beta_1$ approximately takes the form</p>

<div>
$$
\hat{\beta}_1 \pm 2 \cdot SE(\hat{\beta}_1)
$$
</div>

<p>That is, there is approximately a 95% chance that the interval</p>

<div>
$$
\Big[ \hat{\beta}_1 - 2 \cdot SE(\hat{\beta}_1), \hat{\beta}_1 + 2 \cdot SE(\hat{\beta}_1) \Big]
$$
</div>

<p>will contain the true value of $\beta_1$. Similaryly, a confidence interval for $\beta_0$ approximately takes the form</p>

<div>
$$
\hat{\beta}_0 \pm 2 \cdot SE(\hat{\beta}_0)
$$
</div>

<p>Standard errors can also be used to perform <em>hypothesis tests</em> on the coefficients. The most common hypothesis test involves testing the <em>null hypothesis</em> of</p>

<div>
$$
H_0: There\ is\ no\ relationship\ between\ X\ and\ Y
$$
</div>

<p>versus the <em>alternative hypothesis</em></p>

<div>
$$
H_a: There\ is\ some\ relationship\ between\ X\ and\ Y
$$
</div>

<p>Mathematically, this corresponds to testing</p>

<div>
$$
H_0: \beta_1 = 0
$$
</div>

<p>versus</p>

<div>
$$
H_a: \beta_1 \neq 0
$$
</div>

<p>since if $\beta_1 = 0$ then the model reduces to $Y = \beta_0 + \epsilon$, and $X$ is not associated with $Y$. To test the null hypothesis, we need to determine where $\hat{\beta}_1$, our estimate for $\beta_1$, is sufficiently far from zero that we can be confident that $\beta_1$ is non-zero.</p>

<p>How far is far enough? This of course depends on the accuracy of $\hat{\beta}_1$&ndash;that is, it depends on $SE(\hat{\beta}_1)$. If $SE(\hat{\beta}_1)$ is small, then even relatively small values of $\hat{\beta}_1$ may provide strong evidence that $\beta_1 \neq 0$, and hence that there is a relationship between $X$ and $Y$. In contrast, if $SE(\hat{\beta}_1)$ is large, then $\hat{\beta}_1$ must be large in absolute value in order for us to reject the null hypothesis. In practice, we compute a <code>t-statistic</code>, give by</p>

<div>
$$
t = \frac{\hat{\beta}_1 - 0}{SE(\hat{\beta}_1)}
$$
</div>

<p>which measures the number of standard deviations that $\hat{\beta}_1$ is away from 0. If there really is no relationship between $X$ and $Y$, then we expect that the <em>t</em> will have a <em>t</em>-distribution with $n-2$ degrees ofgreedom. The <em>t</em>-distribution has a bell shape and for values of $n$ greater than approximately 30 it is quite similar to normal distribution.</p>

<p>Consequently, it is a simple matter to compute the probability of observing any value equal to $|t|$ or larger, assuming $\beta_1 = 0$. We call this probability the <code>p-value</code>. Roughly speaking, we interpret the <em>p-value</em> as follows: <strong>a small p-value indicates that it is unlikely to ovserve such a substantial association between the predictor and the resonse due to chance, in the absence of any real association between the predictor and the resonse</strong>. Hence, if we see a small p-value, then we can infer that there is an association between the predictor and the response. We <em>reject</em> the null hypothesis &ndash; that is, we declare a relationship to exist between $X$ and $Y$ &ndash; if the p-value is small enough. Typical p-value cutoffs for rejecting the null hypothesis are 5 or 1 %. When $n = 30$, these correspond to t-statistics of around 2 and 2.75, respectively.</p>

<h3 id="assessing-the-accuracy-of-the-model">Assessing the accuracy of the model</h3>

<p>Once we have rejected the null hypothesis in favor of the alternative hypothesis, it is natural to want to quantify the <em>extent to which the model fits the data</em>. The quality of linear regression fit is typically assessed using two related quantities: the <code>residual standard error</code> (RSE) and the $R^2$ statistic.</p>

<p><u><em>Residual Standard Error</em></u></p>

<p>Recall from the model that associated with each observation is an error term $\epsilon$. Due to the presence of these error terms, even if we knew the true regression line, we would not be able to perfectly predict $Y$ from $X$. The <code>RSE</code> is an estimate of the standard deviation of $\epsilon$. Roughly speaking, it is the average amount that the response will deviate from the true regression line. It is computed using the formula</p>

<div>
$$
RSE = \sqrt{\frac{1}{n-2}RSS} = \sqrt{\frac{1}{n-2}\sum_{i=1}^n(y_i-\hat{y}_i)^2}
$$
</div>

<p>Note that <code>RSS</code> was defined as</p>

<div>
$$
RSS=\sum_{i=1}^n (y_i - \hat{y}_i)^2
$$
</div>

<p>The <em>RSE</em> is considered a measure of the <em>lack of fit</em> of the model to the data. If the predictions obtained using the model are very close to the true outcome values &ndash; that is, if $\hat{y}_i \approx y_i$ for $i=1,2,\ldots,n$&ndash;then RSE will be small, and we can conclude that the model fits the data very well. On the other hand, if $\hat{y}_i$ is very far from $y_i$ for one or more observatins, then the RSE may be quite large, indicating that the model doesn&rsquo;t fit the data well.</p>

<p><u><em>The $\underline{R^2}$ Statistic</em></u></p>

<p>The RSE provides an absolute measure of lack of fit of the model to the data. But since it is measured in the units of $Y$, it is not always clear what constitutes a good RSE.</p>

<p>The $R^2$ statistic provides an alternative measure of fit. It takes the form of a proportion&ndash;the proportion of variance explained&ndash;and so it always takes on a value between 0 and 1, and is independent of the scale of $Y$.</p>

<p>To calculate $R^2$, we use the formula</p>

<div>
$$
R^2 = \frac{TSS-RSS}{TSS} = 1 - \frac{RSS}{TSS}
$$
</div>

<p>where $TSS=\sum(y_i - \bar{y})^2$ is the total sum of squares, which measures the total variance in the response $Y$ that can be thought of as the amount of variability inherent in the response before the regression is performed. In contrast, <em>RSS</em> measures the amount of variability that is left unexplained after performing the regression. Hence, $TSS-RSS$ measures the amount of variability in the response that is explained (or removed) by performing the regression, and $R^2$ measures the <em>proportion of variability in Y that can be explained using X</em>. An $R^2$ statistic that is close to 1 indicates that a large proportion of the variability in the response has been explained by the regression. A number near 0 indicates that the regression did not explain mch of the variability in the response; this might occur because the linear model is wrong, or the inherent error $sigma^2$ is high, or both.</p>

<p>The $R^2$ statistic has an interpretational advantage over the <em>RSE</em>, since unlike the <em>RSE</em>, it always lies between 0 and 1. However, it can still be challenging to determine what is a <em>good</em> $R^2$, and in general this will depend on the application.</p>

<p>The $R^2$ statistic is a measure of the linear relationship between $X$ and $Y$. Recall that <em>correlation</em>, defined as</p>

<div>
$$
Cor(X, Y) = \frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2} }
$$
</div>

<p>is also a measure of the linear relationship between $X$ and $Y$. This suggests that we might be able to use $r = Cor(X,Y)$ instead of $R^2$ in order to assess the fit of the linear model. In fact, it can be shown that in the simple linear regression setting, $R^2=r^2$. In other words, the squared correlation and the $R^2$ are identical. However, for the multiple linear regression problem, in which we use several predictors simultaneously to predict the response. The concept of correlation between the predictors and the response does not extend automatically to this setting, since correlation quantifies the association between a single pair of variables rather than between a larger number of variables. We will see that $R^2$ fills this role.</p>

<h2 id="multiple-linear-regression">Multiple Linear Regression</h2>

<p>A simple linear model can be extended to accommodate multiple predictors. We can do this by giving each predictor a separate slope coefficient in a single model. In general, suppose that we have $p$ distanct predictors. Then the multiple linear regression model takes the form</p>

<div>
$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p + \epsilon
$$
</div>

<p>where $X_j$ represents the <em>jth</em> predictor and $\beta_j$ quantifies the association between that variable and the response. We interpret $\beta_j$ as the <em>average</em> effect on $Y$ of a one unit increase in $X_j$, <em>holding all other predictors fixed</em>.</p>

<h3 id="estimating-the-regression-coefficients">Estimating the Regression Coefficients</h3>

<p>The paraeters are estimated using the same least squares approach that we saw in the context of simple linear regression. We choose $\beta_0, \beta_1\ldots,\beta_p$ to minimize the sum of squared residuals.</p>

<div>
$$
\begin{aligned}
RSS &= \sum_{i=1}^n(y_i - \hat{y}_i)^2 \\
&= \sum_{i=1}^n(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_{i1} - \hat{\beta}_2 x_{i2} - \cdots - \hat{\beta}_p x_{ip})^2
\end{aligned}
$$
</div>

<h3 id="some-important-questions">Some Important questions</h3>

<p>When we perform multiple linear regression, we usually are interested in answering a few important questions:</p>

<ol>
<li><p><em>Is at least one of the predictors $X_1, X_2, \ldots, X_p$ useful predicting the response?</em></p></li>

<li><p><em>Do all the predictors help to explain $Y$, or is only a subset of the predictors useful?</em></p></li>

<li><p><em>How well does the model fit the data?</em></p></li>

<li><p><em>Given a set of predictor values, what response value should we predict, and how accuracy is our prediciton?</em></p></li>
</ol>

<p><u>One: Is There a Relationship Between the Response and Predictors?</u></p>

<p>Recall that in the simple linear regression setting, in order to determine whether there is a relationship between the response and the predictor we can simply check whether $\beta_1 = 0$. In the multiple regression setting with $p$ predictors, we need to ask whether all of the regression coefficients are zero, i.e. whether $\beta_1 = \beta-2 = \cdots = \beta_p = 0$. As in the simple linear regression setting, we use a hypothesis test to answer this questions We test the null hypothesis,</p>

<div>
$$
H_0: \beta_1 = \beta_2 = \cdots = \beta_p = 0
$$
</div>

<p>versus the alternative</p>

<div>
$$
H_a: at\ least\ one\ \beta_j\ is\ non-zero
$$
</div>

<p>This hypothesis test is performed by computing the <code>F-statistic</code>,</p>

<div>
$$
F = \frac{(TSS-RSS)/p}{RSS/(n-p-1)},
$$
</div>

<p>where , as with simple linear regression, $TSS=\sum(y_i - \bar{y})^2$ and $RSS=\sum(y_i - \hat{y}_i)^2$.</p>

<p>If the linear model assumptions are correct, one can show that</p>

<div>
$$
E\{ RSS/(n-p-1) \} = \sigma^2
$$
</div>

<p>and that, provided $H_0$ is true,</p>

<div>
$$
E\{ (TSS-RSS)/p \} = \sigma^2
$$
</div>

<p>Hence, when there is <strong>no</strong> relationship between the response and predictors, the <code>F-statistic</code> to take on value close to 1; on the other hand, if $H_a$ is true, then $E{(TSS-RSS)/p} &gt; \sigma^2$, so we expect <code>F-statistic</code> to take on value greater than 1.</p>

<p>How large does the <code>F-statistic</code> need to be before we can reject $H_0$ and conclude that there is a relationship?  It turns out the answer depends on the values of $n$ and $p$.</p>

<p>When $n$ is large, and <code>F-statistic</code> that is just a little larger than 1 might still provide evidence against $H_0$. In contrast, a larger <code>F-statistic</code> is needed to reject $H_0$ if $n$ is small.</p>

<p>When $H_0$ is  true and the error term $\epsilon_i$ have a normal distribution, the <code>F-statistic</code> follows an F-distribution. P-value can be calculated based on the distribution.</p>

<p>Sometimes, we want to test that a particular subset of $q$ of the coefficients are zero. This corresponding to a null hypothesis</p>

<div>
$$
H_0: \beta_{p-q+1} = \beta_{p-q+2} = \ldots = \beta_p = 0
$$
</div>

<p>where for convinience we have put the variables chose for omission at the end of the list. In this case, we fit a second model that uses all the variables <em>except</em> those last $q$. Suppose that the resudual sum of squares for that model is $RSS_0$. Then the appropriate F-statistic is</p>

<div>
$$
F = \frac{(RSS_0 - RSS)/q}{RSS/(n-p-1)}
$$
</div>

<p>Normally, for each individual predictor a <em>t-statistic</em> and a <em>p-value</em> were reported. These provide information about whether each individual predictor is related to the response, after adjusting for the other predictors. It turns out that each of these are exactly equivalent to the F-test that omits that single variable from the model, leaving all the others in. So it reports the <code>partial effect</code> of  adding that variable to the model.</p>

<p>Given these individual p-values for each variable, why do we need to look at the overall F-statistic? After all, it seems likely that if any one of the p-values for the individual variables is very small, then <em>at least one of the predictors is related to the response</em>. However, this logic is flawed, especially when the number of predictors $p$ is large.</p>

<p>For instace, consider an example in which $p = 100$ and $H_0: \beta_1=\beta_2=\ldots=\beta_p=0$ is true, so no variable is truly associated with the response. In this situation, about 5% of the p-values associated with each variable will be below 0.05 by chance. In other words, we expect to see approximately five <em>small</em> p-values even in the absence of any true association between the predictors and hte response. In fact, we are almost guaranteed that we will observe at least one p-value below 0.05 by change! Hence if we use the individual t-statistics and associated p-values in order to decide whether or not there is any association between the variables and the response, there is a very high chance that we will incorrectrly concluded that there is a relationship. However, the F-statistic does not suffer from this problem because it adjusts for the number of predictors. Hence, if $H_0$ is true, there only a 5% chance that the F-statistic will result in a p-value below 0.05 regarless of the number of predictors or the number of observations.</p>

<p>The approach of using an F-statisc to test for any association between the predictors and the response works when $p$ is relative <em>small</em>, and certainly small compared to $n$. However, sometimes we have a very large number of variables. If $p &gt; n$ then there are more coefficients $\beta_j$ to estimate than overvations from which to estimate them. In this case we cannot even fit the multiple linear regressio model using least squares, so the F-statistic cannot be used, and neither can most the other concepts that we have seen so far in this chapter. When $p$ is large, some of the approaches discussed in the next section, such as <code>forward selection</code>, can be used. This <em>high-dimiensional</em> setting is discussed in greater detail in Chapter 6.</p>

<p><u>Two: Deciding on Important Variables</u></p>

<p>As discussed in the previous section, the first step in a multiple regression analysis is to compute the F-statistic and to determine the associated p-values. If we conclude on the basis of that p-values that at least one of the predictors is related to the response, then it is natural to wonder <em>which</em> are the guilty ones! We could look at the individual p-values, but as discussed, if $p$ is large we are likely to make some false discoveries.</p>

<p>It is possible that all of the predictors are associated with the response, but it is more often the case that the response is only related to a subset of the predictors. The task of determining which predictors are associated with the response, in order to fit a single model involving only those predictors, is refered to as <code>variable selection</code>.</p>

<p>Ideally, we would like to perform variable selectoiin by trying out a lot of different models, each containing a different subset of the predictors. For instance, if $p=2$, then we can consider four models: (1) a model containing no variables, (2) a model containing $X_1$ only, (3) a model containing $X_2$ only, (4) a model containing both $X_1$ and $X_2$. We can then select the <em>best</em> model out of all of the models that we have considered. How do we determine which model is best? Various statistics can be used to judge the quality of a model. These include $Mallow&rsquo;s C_p$, $Akaike\ information\ criterion\ (AIC)$, $Bauesian\ information\ criterion\ (BIC)$, and $adjusted\ R^2$. We can also determine which model is best by plotting various model outputs, such as the residuals, in order to search for patterns.</p>

<p>Unfortunately, there are a total of $2^p$ models that contain subsets of $p$ variables. This means that even for moderate $p$, trying out every possible subset of the predictors is infeasible. For instance, we saw that if $p=2$, then there are $2^2 = 4$ models to consider. But if $p=30$, then we must consider $2^30=1,073,741,824$ models! This is not practical.</p>

<p>We need an automated and efficient approach to choose a smaller set of models to consider. There are three classical approaches for this tasks:</p>

<ul>
<li><p><code>Forward selection</code>. We begin with the <em>null model</em> &ndash; a model that contains an intercept but no predictors. We then fit $p$ simple linear regressions and add to the null model the variable that results in the lowes RSS. We then add to that model the variable that results in the lowes RSS for the new tow-variable model. This appraoch is continued until some stopping rule is statisfied.</p></li>

<li><p><code>Backward selection</code>. We start with all variables in the model, and remove the variable with the largest p-value&ndash;that is, the variable that is the least statistically significant. The new $(p-1)$-variable model is fit, and the variable with the largest p-value is removed. This procedure continues until a stopping rule is reached. For instance, we may stop when all remaining variables have a p-value below some threshold.</p></li>

<li><p><code>Mixed selection</code>. This is a combination of forward and backward selection. We start with no variables in the model, and as with forward selection, we add the variable that provides the best fit. We continue to add variables one-by-one. However, if at any point the p-value for one of the variables in the model rises above a certain threshold, then we remove that variable from the model. We continue to perform these foreard and backward steps until all variables in the model have a sufficiently low p-value, and all variables outside the model would have a large p-value if added to the model.</p></li>
</ul>

<p>Backward selection cannot be used if $p &gt; n$, while forward selection can always be used. Forward selection is a greedy approach, and might include variables early that later become redundant. Mixed selection can remedy this.</p>

<p><u>Three: Model Fit</u></p>

<p>Two of the most common numerial measures of model fit are the RSE and $R^2$, the fraction of variance explained.</p>

<p>Recall that in simple regression, $R^2$ is the square of the correlation of the response and the variable. In multiple linear regression, it turns out that it equals $Cor(Y, \hat{Y})^2$, the square of the correlation between the response and fitted linear model; in fact one property of the fitted linear model is that it maximizes this correlation among all possible linear models.</p>

<p>In general RSE is defined as</p>

<div>
$$
RSE = \sqrt{\frac{1}{n-p-1}RSS}
$$
</div>

<p>In addition to looking at the RSE and $R^2$ statistics, it can be useful to plot the data.</p>

<p><u>Four: Predictoins</u></p>

<p>Once we have fit the multiple regression model, it is straightforward to apply in order to predict the response $Y$ on the basis of a set of values for the predictors $X_1,X_2,\ldots,X_p$. However, there are three sorts of uncertainty associated with this prediction.</p>

<ol>
<li>The coefficient estimates $\hat{\beta}_0, \hat{\beta}_1, \ldots,\hat{\beta}_p$ is only an estimate for the <em>true</em> population regression plane. The inacccuracy in the coefficient estimates is related to the <em>reducible error</em>. We can compute a <em>confidence interval</em> in order to determine how close $\hat{Y}$ will be to $f(X)$.</li>
</ol>


        
          <div class="blog-tags">
            
              <a href="https://swang8.github.io/tags/islr/">islr</a>&nbsp;
            
              <a href="https://swang8.github.io/tags/statistics-learning/">statistics learning</a>&nbsp;
            
              <a href="https://swang8.github.io/tags/r/">R</a>&nbsp;
            
          </div>
        

        
            <hr/>
            <section id="social-share">
              <div class="list-inline footer-links">
                

<div class="share-box" aria-hidden="true">
    <ul class="share">
      
      <li>
        <a href="//twitter.com/share?url=https%3a%2f%2fswang8.github.io%2fbooks%2fislr%2fchapter3%2f&amp;text=Chapter3%3a%20Linear%20Regression&amp;via=shichenTX" target="_blank" title="Share on Twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//plus.google.com/share?url=https%3a%2f%2fswang8.github.io%2fbooks%2fislr%2fchapter3%2f" target="_blank" title="Share on Google Plus">
          <i class="fab fa-google-plus"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fswang8.github.io%2fbooks%2fislr%2fchapter3%2f" target="_blank" title="Share on Facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//reddit.com/submit?url=https%3a%2f%2fswang8.github.io%2fbooks%2fislr%2fchapter3%2f&amp;title=Chapter3%3a%20Linear%20Regression" target="_blank" title="Share on Reddit">
          <i class="fab fa-reddit"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2fswang8.github.io%2fbooks%2fislr%2fchapter3%2f&amp;title=Chapter3%3a%20Linear%20Regression" target="_blank" title="Share on LinkedIn">
          <i class="fab fa-linkedin"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.stumbleupon.com/submit?url=https%3a%2f%2fswang8.github.io%2fbooks%2fislr%2fchapter3%2f&amp;title=Chapter3%3a%20Linear%20Regression" target="_blank" title="Share on StumbleUpon">
          <i class="fab fa-stumbleupon"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.pinterest.com/pin/create/button/?url=https%3a%2f%2fswang8.github.io%2fbooks%2fislr%2fchapter3%2f&amp;description=Chapter3%3a%20Linear%20Regression" target="_blank" title="Share on Pinterest">
          <i class="fab fa-pinterest"></i>
        </a>
      </li>
    </ul>
  </div>
  
              </div>
            </section>
        

        
          
          
          <h4 class="see-also">See also</h4>
          <ul>
          
            <li><a href="/books/islr/chapter2/">Chapter2: Basic concepts of Statistical Learning</a></li>
          
          </ul>
          
        
      </article>

      
        <ul class="pager blog-pager">
          
            <li class="previous">
              <a href="https://swang8.github.io/books/islr/chapter2/" data-toggle="tooltip" data-placement="top" title="Chapter2: Basic concepts of Statistical Learning">&larr; Previous Post</a>
            </li>
          
          
        </ul>
      


      
        
        
      

    </div>
  </div>
</div>

    <footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
              <li>
                <a href="mailto:shichen.wang@gmail.com" title="Email me">
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
                <a href="https://github.com/swang8" title="GitHub">
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
                <a href="https://twitter.com/shichenTX" title="Twitter">
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
          
          <li>
            
            <a href="https://swang8.github.io/index.xml" title="RSS">
            
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="fas fa-rss fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
        </ul>
        <p class="credits copyright text-muted">
          
            
              S Wang
            
          

          &nbsp;&bull;&nbsp;&copy;
          
            2019 - 2019
          

          
            &nbsp;&bull;&nbsp;
            <a href="https://swang8.github.io">Deep Trace</a>
          
        </p>
        
        <p class="credits theme-by text-muted">
          <a href="http://gohugo.io">Hugo v0.53</a> powered &nbsp;&bull;&nbsp; Theme by <a href="http://deanattali.com/beautiful-jekyll/">Beautiful Jekyll</a> adapted to <a href="https://github.com/halogenica/beautifulhugo">Beautiful Hugo</a>
          
        </p>
      </div>
    </div>
  </div>
</footer>

<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"></script>
<script src="https://code.jquery.com/jquery-1.12.4.min.js" integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<script src="https://swang8.github.io/js/main.js"></script>
<script src="https://swang8.github.io/js/highlight.min.js"></script>
<script> hljs.initHighlightingOnLoad(); </script>
<script> $(document).ready(function() {$("pre.chroma").css("padding","0");}); </script><script> renderMathInElement(document.body); </script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js" integrity="sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js" integrity="sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q" crossorigin="anonymous"></script>
<script src="https://swang8.github.io/js/load-photoswipe.js"></script>








  </body>
</html>

