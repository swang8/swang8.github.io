<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Introduction to Statistial Learning with applications in R on Deep Trace</title>
    <link>https://swang8.github.io/books/islr/</link>
    <description>Recent content in Introduction to Statistial Learning with applications in R on Deep Trace</description>
    <generator>Hugo -- gohugo.io</generator>
    <managingEditor>shichen.wang@gmail.com (S Wang)</managingEditor>
    <webMaster>shichen.wang@gmail.com (S Wang)</webMaster>
    <lastBuildDate>Wed, 06 Mar 2019 15:22:04 -0600</lastBuildDate>
    
	<atom:link href="https://swang8.github.io/books/islr/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Chapter6 Linear Model Selection and Regularization</title>
      <link>https://swang8.github.io/books/islr/chapter6/</link>
      <pubDate>Wed, 06 Mar 2019 15:22:04 -0600</pubDate>
      <author>shichen.wang@gmail.com (S Wang)</author>
      <guid>https://swang8.github.io/books/islr/chapter6/</guid>
      <description>In the regression setting, the standard linear model
 $$ \tag{6.1}Y = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p + \epsilon $$  is commonly used to describe the relationship between a response $Y$ and a set of variables $X_1, X_2, \ldots, X_p$. We have seen in Chapter 3 that one typiclaly fits this model using least square.
In the chapters that follow, we consider some approaches for extending the linear model framework.</description>
    </item>
    
    <item>
      <title>Chapter5 Resampling Methods</title>
      <link>https://swang8.github.io/books/islr/chapter5/</link>
      <pubDate>Fri, 01 Mar 2019 16:17:55 -0600</pubDate>
      <author>shichen.wang@gmail.com (S Wang)</author>
      <guid>https://swang8.github.io/books/islr/chapter5/</guid>
      <description>Resampling methods are an indispensable tool in modern statistics. They involve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information abou the fitted model. For example, in order to estimate the variability of a linear regression fit, we can repeatedly draw different samples from the training data, fit a linear regression to each new sample, and then examine the extent to which the resulting fits differ.</description>
    </item>
    
    <item>
      <title>Chapter4 Classification</title>
      <link>https://swang8.github.io/books/islr/chapter4/</link>
      <pubDate>Mon, 18 Feb 2019 09:19:59 -0600</pubDate>
      <author>shichen.wang@gmail.com (S Wang)</author>
      <guid>https://swang8.github.io/books/islr/chapter4/</guid>
      <description>In this chapter, we study approaches for predicting qualitative responses, a process that is known as classification. Predicting a qualitative response for an obervation can be refered to as classifying that observation, since it involves assigning the observation to a category, or class. On the other hand, often the methods used for classification first predict the probability of each of the categories of a qualitative variable, as the basis for making the classification.</description>
    </item>
    
    <item>
      <title>Chapter3: Linear Regression</title>
      <link>https://swang8.github.io/books/islr/chapter3/</link>
      <pubDate>Thu, 07 Feb 2019 14:46:09 -0600</pubDate>
      <author>shichen.wang@gmail.com (S Wang)</author>
      <guid>https://swang8.github.io/books/islr/chapter3/</guid>
      <description>Linear Regression Linear regression is a very simple supervised learning methods, though still very useful.
Simple Linear Regression Simple linear regression is a straightforward approach for predicting a quantitative response $Y$ on the basis of a single predictor variable $X$. It assumes that there is approximately a linear relationship between $X$ and $Y$.
 $$ Y \approx \beta_{0} + \beta_{1}X $$  In the equation, $\beta_0$ and $\beta_1$ are two unknown constants that represetn the intercept and slope termes in the linear model.</description>
    </item>
    
    <item>
      <title>Chapter2: Basic concepts of Statistical Learning</title>
      <link>https://swang8.github.io/books/islr/chapter2/</link>
      <pubDate>Fri, 01 Feb 2019 14:46:09 -0600</pubDate>
      <author>shichen.wang@gmail.com (S Wang)</author>
      <guid>https://swang8.github.io/books/islr/chapter2/</guid>
      <description>Statistical Learning What is statistial learning Suppose we observe a quantitative response $Y$ and $p$ different predictors, $X_1, X_2, \ldots,X_p$ . We assume that there is a relationship between $Y$ and $X=(X_1, X_2,\ldots,X_p)$, which can be written as
 $$ Y=f(X)+\epsilon $$  Here $f$ is some fixed but unknown fucntion of $X_1,X_2,\ldots,X_p$, and $\epsilon$ is a random error term, which is independent of $X$ and has mean zero. In this formulation, $f$ represents the systematic informationa $X$ provides about $Y$.</description>
    </item>
    
    <item>
      <title>Introduction</title>
      <link>https://swang8.github.io/books/islr/intro/</link>
      <pubDate>Wed, 30 Jan 2019 00:00:00 +0000</pubDate>
      <author>shichen.wang@gmail.com (S Wang)</author>
      <guid>https://swang8.github.io/books/islr/intro/</guid>
      <description>An overview of statistial learning Statistical learning refers to a vast set of tools for understanding data.
Two categories: supervised and unsupervised.
Supervised: Build models based on known input and output data, then use the model for prediction or estimation.
Unsupervised: There are inputs but no supervised outputs. We can learn relationships and structures from such data.
Notation and simple algebra Let the $X$ denotes a matrix. $X_{ij}$ represents the value of row $i$ and column $j$.</description>
    </item>
    
  </channel>
</rss>